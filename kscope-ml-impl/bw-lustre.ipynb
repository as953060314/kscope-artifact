{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MKL_THREADING_LAYER\"] = \"GNU\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import pymc3 as pm\n",
    "import scipy.stats as ss\n",
    "from sampled import sampled\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import pickle\n",
    "theano.config.exception_verbosity='high'\n",
    "\n",
    "#disable logging temporarily\n",
    "import logging\n",
    "logger = logging.getLogger(\"pymc3\")\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LustrePartition:\n",
    "    def __init__(self, name, num_osts, s_prob, d_prob, model):\n",
    "        self.name = name\n",
    "        self.num_osts = num_osts\n",
    "        self.num_oss_servers = num_osts\n",
    "        self.num_mds = 1\n",
    "        \n",
    "        # topology\n",
    "        self.net1_paths = None\n",
    "        self.net2_paths = None\n",
    "        \n",
    "        # potential\n",
    "        self.net1_potential = None\n",
    "        self.net2_potential = None\n",
    "        \n",
    "        #pm.Beta('c_prob', alpha=10.0, beta =0.1)\n",
    "        with model:\n",
    "            self.oss_servers = pm.Bernoulli(name + '_oss', p = s_prob, shape = self.num_oss_servers)\n",
    "            self.osts = pm.Bernoulli(name + '_ost', p = d_prob, shape = self.num_osts)\n",
    "            self.mds_server = pm.Bernoulli(name + '_mds',   p = s_prob)\n",
    "            self.ha_disk_paths = tt._shared(np.asarray([0.0]*self.num_oss_servers, dtype=theano.config.floatX))\n",
    "            for ha_disk in range(self.num_oss_servers):\n",
    "                self.ha_disk_paths = tt.set_subtensor(self.ha_disk_paths[ha_disk], self.__create_ha__(ha_disk))\n",
    "            self.ha_pairs = tt.reshape(self.ha_disk_paths, (1,self.num_oss_servers))\n",
    "            \n",
    "    def __create_ha__(self, ha_disk):\n",
    "        # series parallel system for ha_server\n",
    "        if ha_disk % 2 == 0:\n",
    "            return (1.0 - ((1.0-self.oss_servers[ha_disk]) *  (1.0-self.oss_servers[ha_disk+1]))) * self.osts[ha_disk]\n",
    "        else:\n",
    "            return (1.0 - ((1.0-self.oss_servers[ha_disk]) *  (1.0-self.oss_servers[ha_disk-1]))) * self.osts[ha_disk]\n",
    "\n",
    "\n",
    "class Kaleidoscope:\n",
    "    def __init__(self):\n",
    "        # pymc3 params\n",
    "        self.num_draws = 100\n",
    "        self.num_tune = 0\n",
    "        self.num_cores = 10\n",
    "        self.c_prob = 0.999\n",
    "        self.s_prob = 0.999\n",
    "        self.d_prob = 0.999\n",
    "        self.n_prob = 0.99\n",
    "        self.pm_model = pm.Model()\n",
    "        self.health_status = None\n",
    "        \n",
    "        # monitoring params\n",
    "        self.num_clients = 6 # store ping clients\n",
    "        \n",
    "        # Blue Waters design\n",
    "        self.lustre_partitions = {\n",
    "            \"scratch\":  LustrePartition(\"scratch\", 360, self.s_prob, self.d_prob, self.pm_model), \n",
    "            \"projects\": LustrePartition(\"projects\", 36, self.s_prob, self.d_prob, self.pm_model),\n",
    "            \"home\":     LustrePartition(\"home\", 36, self.s_prob, self.d_prob, self.pm_model)\n",
    "        }\n",
    "        # define lustre components\n",
    "        with self.pm_model:\n",
    "            # define networks\n",
    "            self.ib_net = pm.Bernoulli('ib_net', p = self.n_prob)                         # 1\n",
    "            self.gem_net = pm.Bernoulli('gem_net', p = self.n_prob)\n",
    "            \n",
    "            # define clients\n",
    "            self.clients = pm.Bernoulli(\"clients\", p = self.c_prob, shape = self.num_clients)\n",
    "            for partition in self.lustre_partitions:\n",
    "                store_partition = self.lustre_partitions[partition] # aliasing\n",
    "                # define the topology network of storage system # c -> n (..) -> mds -> ha -> mdt\n",
    "                store_partition.net1_paths = \\\n",
    "                    pm.math.dot(\n",
    "                        tt.reshape(\n",
    "                            self.clients[:self.num_clients-1]* self.ib_net * store_partition.mds_server, (self.num_clients-1, 1)\n",
    "                        ), store_partition.ha_pairs\n",
    "                    )\n",
    "                store_partition.net2_paths = \\\n",
    "                    pm.math.dot(\n",
    "                        tt.reshape(\n",
    "                            self.clients[self.num_clients-1] * self.ib_net * self.gem_net * store_partition.mds_server, (1, 1) # only one client connected on this network\n",
    "                        ), store_partition.ha_pairs\n",
    "                    )\n",
    "                \n",
    "        \n",
    "    def __apply_observations__(self, p_mat, data_success, data_fail):\n",
    "        return tt.log(\n",
    "            (\n",
    "                tt.reshape(pm.math.flatten(p_mat), (1, -1))* \n",
    "                tt.reshape(tt._shared(data_success), (1, -1)) + 1e-9\n",
    "            )  + \n",
    "             tt.reshape(pm.math.flatten(1.-p_mat), (1,-1))* \n",
    "                tt.reshape(tt._shared(data_fail), (1,-1)) + 1e-9\n",
    "        ) \n",
    "    \n",
    "    def infer(self, o_data):\n",
    "        with self.pm_model:\n",
    "            for partition in o_data:\n",
    "                d1_s, d1_f, d2_s, d2_f = o_data[partition]['d1_s'], o_data[partition]['d1_f'], o_data[partition]['d2_s'], o_data[partition]['d2_f']\n",
    "                # likelihood function\n",
    "                if partition in self.lustre_partitions:\n",
    "                    store_partition = self.lustre_partitions[partition]\n",
    "                    store_partition.net1_potential = pm.Potential(partition + '_paths_net1', self.__apply_observations__(store_partition.net1_paths, d1_s, d1_f))\n",
    "                    store_partition.net2_potential = pm.Potential(partition + '_paths_net2', self.__apply_observations__(store_partition.net2_paths, d2_s, d2_f))\n",
    "            self.health_status = pm.sample(draws=self.num_draws, tune = self.num_tune, cores=self.num_cores, progressbar=False)\n",
    "\n",
    "    def print_health_stats(self):\n",
    "        with self.pm_model:\n",
    "            # print all client status\n",
    "            for i in range(0,self.num_clients):\n",
    "                client_status =  np.sum(self.health_status.get_values('clients')[:,i])/float(len(self.health_status.get_values('clients')[:,i]))\n",
    "                if client_status < 0.8:\n",
    "                    print(\"client \" + str(i) +  \" :\", client_status)\n",
    "            # print network status\n",
    "            ib_status = np.sum(self.health_status.get_values('ib_net'))/float(len(self.health_status.get_values('ib_net')))\n",
    "            if ib_status < 0.8:\n",
    "                print(\"ib_net \", ib_status )\n",
    "            gem_status = np.sum(self.health_status.get_values('ib_net'))/float(len(self.health_status.get_values('ib_net')))\n",
    "            if gem_status < 0.8:\n",
    "                print(\"gem \", gem_status)\n",
    "            \n",
    "            #print partition status only failed ones\n",
    "            for partition in self.lustre_partitions:\n",
    "                mds_status = np.sum(self.health_status.get_values(partition + '_mds'))/float(len(self.health_status.get_values(partition + '_mds')))\n",
    "                if mds_status < 0.8:\n",
    "                    print(partition + \"_mds \" +  \" :\", mds_status)\n",
    "                for i in range(0,self.lustre_partitions[partition].num_oss_servers):\n",
    "                    p = np.sum(self.health_status.get_values(partition + '_oss')[:,i])/float(len(self.health_status.get_values(partition + '_oss')[:,i]))\n",
    "                    if p <=0.95:\n",
    "                        print(partition + \"_oss \" + str(i) +  \" :\", p)\n",
    "                for i in range(0,self.lustre_partitions[partition].num_osts):\n",
    "                    p = np.sum(self.health_status.get_values(partition + '_ost')[:,i])/float(len(self.health_status.get_values(partition + '_ost')[:,i]))\n",
    "                    if p <=0.95:\n",
    "                        print(partition + \"_ost \" + str(i) +  \" :\", p)\n",
    "    def print_health_stats_trace(self, ts):\n",
    "        with self.pm_model:\n",
    "            # print all client status\n",
    "            for i in range(0,self.num_clients):\n",
    "                client_status =  np.sum(self.health_status.get_values('clients')[:,i])/float(len(self.health_status.get_values('clients')[:,i]))\n",
    "                if client_status < 0.8:\n",
    "                    print(ts, \"client \" + str(i) +  \" :\", client_status)\n",
    "            # print network status\n",
    "            ib_status = np.sum(self.health_status.get_values('ib_net'))/float(len(self.health_status.get_values('ib_net')))\n",
    "            if ib_status < 0.8:\n",
    "                print(ts, \"ib_net \", ib_status )\n",
    "            gem_status = np.sum(self.health_status.get_values('ib_net'))/float(len(self.health_status.get_values('ib_net')))\n",
    "            if gem_status < 0.8:\n",
    "                print(ts, \"gem \", gem_status)\n",
    "            \n",
    "            #print partition status only failed ones\n",
    "            for partition in self.lustre_partitions:\n",
    "                mds_status = np.sum(self.health_status.get_values(partition + '_mds'))/float(len(self.health_status.get_values(partition + '_mds')))\n",
    "                if mds_status < 0.8:\n",
    "                    print(ts, partition + \"_mds \" +  \" :\", mds_status)\n",
    "                for i in range(0,self.lustre_partitions[partition].num_oss_servers):\n",
    "                    p = np.sum(self.health_status.get_values(partition + '_oss')[:,i])/float(len(self.health_status.get_values(partition + '_oss')[:,i]))\n",
    "                    if p <=0.95:\n",
    "                        print(ts, partition + \"_oss \" + str(i) +  \" :\", p)\n",
    "                for i in range(0,self.lustre_partitions[partition].num_osts):\n",
    "                    p = np.sum(self.health_status.get_values(partition + '_ost')[:,i])/float(len(self.health_status.get_values(partition + '_ost')[:,i]))\n",
    "                    if p <=0.95:\n",
    "                        print(ts, partition + \"_ost \" + str(i) +  \" :\", p)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTrace:\n",
    "    def __init__(self, ddir):\n",
    "        self.num_clients_net1 = 5\n",
    "        self.num_clients_net2 = 1\n",
    "        self.lustre_partitions = {\n",
    "            \"scratch\":  {\"num_oss\" : 360, 'd_s': None, 'd_f': None, \"d1_s\": None, \"d1_f\": None, \"d2_s\": None, \"d2_f\": None }, \n",
    "            \"projects\": {\"num_oss\" : 36, 'd_s': None, 'd_f': None, \"d1_s\": None, \"d1_f\": None, \"d2_s\": None, \"d2_f\": None },\n",
    "            \"home\":     {\"num_oss\" : 36, 'd_s': None, 'd_f': None, \"d1_s\": None, \"d1_f\": None, \"d2_s\": None, \"d2_f\": None}\n",
    "        }\n",
    "        self.ddir = ddir\n",
    "        \n",
    "    def run_trace(self):\n",
    "        scratch_files = [f.split(\".\")[0] for f in os.listdir(self.ddir + os.sep + \"scratch\")]\n",
    "        project_files = [f.split(\".\")[0] for f in os.listdir(self.ddir + os.sep + \"projects\")]\n",
    "        home_files =    [f.split(\".\")[0] for f in os.listdir(self.ddir + os.sep + \"home\")]\n",
    "        \n",
    "        for f in scratch_files:\n",
    "            for partition in self.lustre_partitions:\n",
    "                store_partition = self.lustre_partitions[partition]\n",
    "                fname = self.ddir + os.sep + partition + os.sep + f\n",
    "                store_partition['d_s'] = pickle.load(open(fname + \".success\", 'rb'))\n",
    "                store_partition['d_f'] = pickle.load(open(fname + \".failed\", 'rb'))\n",
    "            for ts in self.lustre_partitions['scratch']['d_s']:\n",
    "                try:\n",
    "                    for partition in self.lustre_partitions:\n",
    "                        store_partition = self.lustre_partitions[partition]\n",
    "                        num_oss = store_partition['num_oss']\n",
    "                        d_s = store_partition['d_s']\n",
    "                        d_f = store_partition['d_f']\n",
    "\n",
    "                        d1_s = d_s[ts][0 : 5 * num_oss]\n",
    "                        d2_s = d_s[ts][5 * num_oss: 6*num_oss]\n",
    "                        d1_f = d_f[ts][0 : 5*num_oss]\n",
    "                        d2_f = d_f[ts][5 * num_oss: 6*num_oss]\n",
    "                        store_partition['d1_s'], store_partition['d1_f'], store_partition['d2_s'], store_partition['d2_f'] = \\\n",
    "                            d1_s, d1_f, d2_s, d2_f\n",
    "                except:\n",
    "#                     print(\"discarded the dataset as %d not found in %s\" % (ts, partition)) # TODO\n",
    "                    continue\n",
    "                k = Kaleidoscope()\n",
    "                k.infer(self.lustre_partitions)\n",
    "                k.print_health_stats_trace(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test:\n",
    "    def __init__(self, failed_componnet_id):\n",
    "        self.num_clients_net1 = 5\n",
    "        self.num_clients_net2 = 1\n",
    "        self.failed_componnet_id = failed_componnet_id\n",
    "        self.lustre_partitions = {\n",
    "            \"scratch\":  {\"num_oss\" : 360, 'd1_s': None, 'd1_f': None, 'd2_s': None, 'd2_f': None}, \n",
    "            \"projects\": {\"num_oss\" : 36, 'd1_s': None, 'd1_f': None, 'd2_s': None, 'd2_f': None},\n",
    "            \"home\":     {\"num_oss\" : 36, 'd1_s': None, 'd1_f': None, 'd2_s': None, 'd2_f': None}\n",
    "        }\n",
    "        for partition in self.lustre_partitions:\n",
    "            store_partition = self.lustre_partitions[partition]\n",
    "            store_partition['d1_s'], store_partition['d1_f'], store_partition['d2_s'], store_partition['d2_f'] = \\\n",
    "                self.__generate_test_data__(store_partition['num_oss'])\n",
    "        k = Kaleidoscope()    \n",
    "        k.infer(self.lustre_partitions)\n",
    "        k.print_health_stats()\n",
    "        \n",
    "    def __generate_test_data__(self, num_oss):\n",
    "        # dummy data of observations\n",
    "        num_samples = 10\n",
    "        eps = 1e-9\n",
    "        dummy_data_success_net1 = np.array([num_samples]*num_oss*self.num_clients_net1)\n",
    "        dummy_data_fail_net1 = np.array([eps]*num_oss*self.num_clients_net1)\n",
    "\n",
    "        dummy_data_success_net2 = np.array([num_samples]*num_oss*1)\n",
    "        dummy_data_fail_net2 = np.array([eps]*num_oss*1)\n",
    "\n",
    "        for i in range(self.num_clients_net1):\n",
    "            dummy_data_success_net1[i*num_oss + self.failed_componnet_id] = eps\n",
    "            dummy_data_fail_net1[i*num_oss + self.failed_componnet_id] = num_samples\n",
    "\n",
    "        dummy_data_success_net2[0*num_oss + self.failed_componnet_id] = eps\n",
    "        dummy_data_fail_net2[0*num_oss + self.failed_componnet_id] = num_samples\n",
    "        return dummy_data_success_net1, dummy_data_fail_net1, dummy_data_success_net2, dummy_data_fail_net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt = DataTrace( \"/gpfs/gpfs0/home/sjha8/projects/klscope/data_mat\")\n",
    "# dt.run_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 100 samples in chain.\n",
      "Multiprocess sampling (10 chains in 10 jobs)\n",
      "BinaryGibbsMetropolis: [scratch_oss, scratch_ost, scratch_mds, projects_oss, projects_ost, projects_mds, home_oss, home_ost, home_mds, ib_net, gem_net, clients]\n",
      "The rhat statistic is larger than 1.05 for some parameters. This indicates slight problems during sampling.\n",
      "The estimated number of effective samples is smaller than 200 for some parameters.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scratch_ost 3 : 0.002\n",
      "projects_ost 3 : 0.005\n",
      "home_ost 3 : 0.007\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "# failing ost 3 on each partition\n",
    "testgen = Test(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
